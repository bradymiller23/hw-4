false_negative_rate = false_negative_rate
)
)
}
overview(df$y, df$y)
# getting predictions on the survival of Titanic passengers
predictions <- predict(full_model, type = 'response')
predictions <- ifelse(predictions >= 0.5, 1, 0)
# getting expected value for survival (survived = 1, died = 0)
expected <- df$y
# using overview func we created to see key performance metrics of full_model
fullModel_overview <- overview(predictions, expected)
fullModel_overview
# creating null model which backwards stepwise logistic regression will end at
null_model <- glm(y ~ 1, df, family = binomial())
# creating the backwards stepwise logistic regression model
step_model <- step(full_model, direction = 'backward',
scope = formula(null_model))
summary(step_model)
# creating predictions based on the backward stepwise logistic regression model
step_predictions <- predict(step_model, type = 'response')
# ifelse statement to classify the predictions since they are currently decimals
step_predictions <- ifelse(step_predictions >= 0.5, 1, 0)
stepwise_overview <- overview(step_predictions, df$y)
stepwise_overview
controls <- trainControl(method = 'cv', number = 5)
lasso_fit <- train(
y ~ pclass + sex + age + `siblings spouses aboard` +
`parents children aboard` + fare,
data = df,
method = 'glmnet',
trControl = controls,
tuneGrid = expand.grid(
alpha = 1,
lambda = 2^seq(-20, 0, by = 0.5)
),
standardize = TRUE,
family = 'binomial'
)
# plotting the log2(lambda) values vs accuracy to find optimal lambda
lasso_fit$results %>%
ggplot() +
geom_line(aes(x = log2(lambda), y = Accuracy)) +
xlab('log_2(lambda)') +
ylab('Cross-Validation Accuracy')
# showing where max accuracy is &  the associated lambda (the optimal lambda)
lasso_fit$results %>%
filter(Accuracy == max(lasso_fit$results$Accuracy))
# creating predictions based on lambda model
lasso_predictions <- predict(lasso_fit)
lasso_overview <- overview(lasso_predictions ,df$y)
lasso_overview
covariate_matrix <- model.matrix(full_model)[, -1]
X <- torch_tensor(covariate_matrix, dtype = torch_float())
y <- torch_tensor(df$y, dtype = torch_float())
logistic <- nn_module(
initialize = function() {
# need 7 in the linear layer because pclass is being used as a factor
self$f <- nn_linear(7,1)
self$g <- nn_sigmoid()
},
forward = function(x) {
x %>%
self$f() %>%
self$g()
}
)
f <- logistic()
f(X)
Loss <- function(X, y, Fun){
nn_bce_loss()(Fun(X), y)
}
func <- function(z){
(z^4) - 6*(z)^2 - 3*z + 4
}
z_0 <- torch_tensor(-3.5, requires_grad = TRUE)
output <- f(z_0)
h <- function(u,v){
(torch_dot(u,v))^3
}
u <- torch_tensor(c(-1,1,-1,1,-1,1,-1,1,-1,1), requires_grad = TRUE)
v <- torch_tensor(c(-1,-1,-1,-1,-1,1,1,1,1,1), requires_grad = TRUE)
result <- h(u,v)
result$backward()
u$grad
func <- function(z){
(z^4) - 6*(z)^2 - 3*z + 4
}
z_0 <- torch_tensor(-3.5, requires_grad = TRUE)
output <- f(z_0)
func <- function(z){
(z^4) - 6*(z)^2 - 3*z + 4
}
z_0 <- torch_tensor(-3.5, requires_grad = TRUE)
output <- func(z_0)
output$backward()
z_0$grad
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.02
z_values <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
z_values <- c(z_values, z)
}
# creating a x-axis boundary
x_values <- seq(-4,0, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
y_values <- func(x_values)
# creating data frames to store the function values and the gradient desc values
df_f <- data.frame(x = x_values, y = y_values)
df_z <- data.frame(x = z_values, y = f(z_values))
# creating a x-axis boundary
x_values <- seq(-4,0, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
y_values <- func(x_values)
# creating data frames to store the function values and the gradient desc values
df_f <- data.frame(x = x_values, y = y_values)
df_z <- data.frame(x = z_values, y = func(z_values))
ggplot() +
# red line showing the actual curve of the line
geom_line(data = df_f, aes(x=x, y=y), color = 'red', size = 1) +
# blue points showing the value after each iteration of gradient descent
geom_point(data = df_z, aes(x=x, y=y), color = 'blue', size = 3) +
xlab('z') +
ylab('f(z)') +
ggtitle('Gradient Descent for f(z)')
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.03
zvals <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
zvals <- c(zvals, z)
}
# creating a x-axis boundary
xvals <- seq(-4,4, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
yvals <- f(xvals)
packages <- c(
"dplyr",
"readr",
"tidyr",
"purrr",
"stringr",
"corrplot",
"car",
"caret",
"torch",
"nnet",
"broom"
)
renv::install(packages)
sapply(packages, require, character.only=T)
library(numDeriv)
g <- function(x) {
(x[1] - 3)^2 + (x[2] - 4)^2
}
grad(g, c(3,4))
# command not working so commenting out in order to load PDF
#$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$
# command also not working for pdf
#$$
#\begin{aligned}
#\frac{d}{du}\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), #\dots, \frac{d}{du_n}h(\u, \v)\Bigg)
#\end{aligned}
#$$
# command not working for pdf
#$$
#\begin{aligned}
#\frac{d}{du}\nabla_\u h(\u, \v) &= \Bigg(3(\u \cdot \v)^2 \times v_1, 3(\u \cdot \v)^2 #\times v_2, \dots, 3(\u \cdot \v)^2 \times v_n\Bigg)\
#\end{aligned}
#$$
# command not working as desired so commenting it out
#$$
#\begin{aligned}
#\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
#\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
#\end{aligned}
#$$
h <- function(u,v){
(torch_dot(u,v))^3
}
u <- torch_tensor(c(-1,1,-1,1,-1,1,-1,1,-1,1), requires_grad = TRUE)
v <- torch_tensor(c(-1,-1,-1,-1,-1,1,1,1,1,1), requires_grad = TRUE)
result <- h(u,v)
result$backward()
u$grad
func <- function(z){
(z^4) - 6*(z)^2 - 3*z + 4
}
z_0 <- torch_tensor(-3.5, requires_grad = TRUE)
output <- func(z_0)
output$backward()
z_0$grad
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.02
z_values <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
z_values <- c(z_values, z)
}
# creating a x-axis boundary
x_values <- seq(-4,0, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
y_values <- func(x_values)
# creating data frames to store the function values and the gradient desc values
df_f <- data.frame(x = x_values, y = y_values)
df_z <- data.frame(x = z_values, y = func(z_values))
ggplot() +
# red line showing the actual curve of the line
geom_line(data = df_f, aes(x=x, y=y), color = 'red', size = 1) +
# blue points showing the value after each iteration of gradient descent
geom_point(data = df_z, aes(x=x, y=y), color = 'blue', size = 3) +
xlab('z') +
ylab('f(z)') +
ggtitle('Gradient Descent for f(z)')
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.03
zvals <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
zvals <- c(zvals, z)
}
# creating a x-axis boundary
xvals <- seq(-4,4, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
yvals <- f(xvals)
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.03
zvals <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
zvals <- c(zvals, z)
}
# creating a x-axis boundary
xvals <- seq(-4,4, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
yvals <- func(xvals)
# creating data frames to store the function values and the gradient desc values
df_f <- data.frame(x = xvals, y = yvals)
df_z <- data.frame(x = zvals, y = func(zvals))
ggplot() +
# red line showing the actual curve of the line
geom_line(data = df_f, aes(x=x, y=y), color = 'red', size = 1) +
# blue points showing the value after each iteration of gradient descent
geom_point(data = df_z, aes(x=x, y=y), color = 'blue', size = 3) +
xlab('z') +
ylab('f(z)') +
ggtitle('Gradient Descent for f(z)')
packages <- c(
"dplyr",
"readr",
"tidyr",
"purrr",
"stringr",
"corrplot",
"car",
"caret",
"torch",
"nnet",
"broom"
)
renv::install(packages)
sapply(packages, require, character.only=T)
library(numDeriv)
g <- function(x) {
(x[1] - 3)^2 + (x[2] - 4)^2
}
grad(g, c(3,4))
# command not working so commenting out in order to load PDF
#$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$
# command also not working for pdf
#$$
#\begin{aligned}
#\frac{d}{du}\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), #\dots, \frac{d}{du_n}h(\u, \v)\Bigg)
#\end{aligned}
#$$
# command not working for pdf
#$$
#\begin{aligned}
#\frac{d}{du}\nabla_\u h(\u, \v) &= \Bigg(3(\u \cdot \v)^2 \times v_1, 3(\u \cdot \v)^2 #\times v_2, \dots, 3(\u \cdot \v)^2 \times v_n\Bigg)\
#\end{aligned}
#$$
# command not working as desired so commenting it out
#$$
#\begin{aligned}
#\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
#\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
#\end{aligned}
#$$
h <- function(u,v){
(torch_dot(u,v))^3
}
u <- torch_tensor(c(-1,1,-1,1,-1,1,-1,1,-1,1), requires_grad = TRUE)
v <- torch_tensor(c(-1,-1,-1,-1,-1,1,1,1,1,1), requires_grad = TRUE)
result <- h(u,v)
result$backward()
u$grad
func <- function(z){
(z^4) - 6*(z)^2 - 3*z + 4
}
z_0 <- torch_tensor(-3.5, requires_grad = TRUE)
output <- func(z_0)
output$backward()
z_0$grad
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.02
z_values <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
z_values <- c(z_values, z)
}
# creating a x-axis boundary
x_values <- seq(-4,0, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
y_values <- func(x_values)
# creating data frames to store the function values and the gradient desc values
df_f <- data.frame(x = x_values, y = y_values)
df_z <- data.frame(x = z_values, y = func(z_values))
ggplot() +
# red line showing the actual curve of the line
geom_line(data = df_f, aes(x=x, y=y), color = 'red', size = 1) +
# blue points showing the value after each iteration of gradient descent
geom_point(data = df_z, aes(x=x, y=y), color = 'blue', size = 3) +
xlab('z') +
ylab('f(z)') +
ggtitle('Gradient Descent for f(z)')
# initializing parameters to be used for the function
z <- -3.5
n <- 100
eta <- 0.03
zvals <- c(z)
for (i in 1:n) {
# updates derivative each iteration & finds new z value, adding it to array
deriv <- 4*z^3 - 12*z - 3
z <- z - eta * deriv
zvals <- c(zvals, z)
}
# creating a x-axis boundary
xvals <- seq(-4,4, by = 0.01)
# finding y-values for those x values (finding f(x) for function f above)
yvals <- func(xvals)
# creating data frames to store the function values and the gradient desc values
df_f <- data.frame(x = xvals, y = yvals)
df_z <- data.frame(x = zvals, y = func(zvals))
ggplot() +
# red line showing the actual curve of the line
geom_line(data = df_f, aes(x=x, y=y), color = 'red', size = 1) +
# blue points showing the value after each iteration of gradient descent
geom_point(data = df_z, aes(x=x, y=y), color = 'blue', size = 3) +
xlab('z') +
ylab('f(z)') +
ggtitle('Gradient Descent for f(z)')
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
df <- read.csv(url)
# changing column name of 'Survived'
colnames(df)[colnames(df) == 'Survived'] <- 'y'
# getting rid of . in the column names
colnames(df)[colnames(df)=='Siblings.Spouses.Aboard']<-'Siblings Spouses Aboard'
colnames(df)[colnames(df)=='Parents.Children.Aboard']<-'Parents Children Aboard'
# converting all the column names to lower case
colnames(df) <- tolower(colnames(df))
# changing the y (survived) and sex to factors
df$y <- as.factor(df$y)
df$sex <- as.factor(ifelse(df$sex == 'male', 1, 0))
# converting pclass to factor too even though not a binary variable
# makes more sense when interpreting slope
df$pclass <- as.factor(df$pclass)
# keeping only numeric values
dfCorr <- df %>%
keep(is.numeric) %>%
cor()
# creating corrplot
corrplot(dfCorr, method = 'color', tl.cex = 0.9, tl.col = 'black',
order = 'hclust')
full_model <-  glm(y ~ pclass + sex + age + `siblings spouses aboard` +
`parents children aboard` + fare, data = df,
family = binomial())
summary(full_model)
overview <- function(predicted, expected){
total_false_positives <- sum(expected != predicted & expected == 0)
total_true_positives <- sum(expected == predicted & expected == 1)
total_false_negatives <- sum(expected != predicted & expected == 1)
total_true_negatives <- sum(expected == predicted & expected == 0)
false_positive_rate <- total_false_positives / (total_false_positives +
total_true_negatives)
false_negative_rate <- total_false_negatives / (total_false_negatives +
total_true_positives)
accuracy <- (total_true_positives + total_true_negatives) / length(expected)
error <- (total_false_positives + total_false_negatives) / length(expected)
return(
data.frame(
accuracy = accuracy,
error=error,
false_positive_rate = false_positive_rate,
false_negative_rate = false_negative_rate
)
)
}
overview(df$y, df$y)
# getting predictions on the survival of Titanic passengers
predictions <- predict(full_model, type = 'response')
predictions <- ifelse(predictions >= 0.5, 1, 0)
# getting expected value for survival (survived = 1, died = 0)
expected <- df$y
# using overview func we created to see key performance metrics of full_model
fullModel_overview <- overview(predictions, expected)
fullModel_overview
# creating null model which backwards stepwise logistic regression will end at
null_model <- glm(y ~ 1, df, family = binomial())
# creating the backwards stepwise logistic regression model
step_model <- step(full_model, direction = 'backward',
scope = formula(null_model))
summary(step_model)
# creating predictions based on the backward stepwise logistic regression model
step_predictions <- predict(step_model, type = 'response')
# ifelse statement to classify the predictions since they are currently decimals
step_predictions <- ifelse(step_predictions >= 0.5, 1, 0)
stepwise_overview <- overview(step_predictions, df$y)
stepwise_overview
controls <- trainControl(method = 'cv', number = 5)
lasso_fit <- train(
y ~ pclass + sex + age + `siblings spouses aboard` +
`parents children aboard` + fare,
data = df,
method = 'glmnet',
trControl = controls,
tuneGrid = expand.grid(
alpha = 1,
lambda = 2^seq(-20, 0, by = 0.5)
),
standardize = TRUE,
family = 'binomial'
)
# plotting the log2(lambda) values vs accuracy to find optimal lambda
lasso_fit$results %>%
ggplot() +
geom_line(aes(x = log2(lambda), y = Accuracy)) +
xlab('log_2(lambda)') +
ylab('Cross-Validation Accuracy')
# showing where max accuracy is &  the associated lambda (the optimal lambda)
lasso_fit$results %>%
filter(Accuracy == max(lasso_fit$results$Accuracy))
# creating predictions based on lambda model
lasso_predictions <- predict(lasso_fit)
lasso_overview <- overview(lasso_predictions ,df$y)
lasso_overview
covariate_matrix <- model.matrix(full_model)[, -1]
X <- torch_tensor(covariate_matrix, dtype = torch_float())
y <- torch_tensor(df$y, dtype = torch_float())
logistic <- nn_module(
initialize = function() {
# need 7 in the linear layer because pclass is being used as a factor
self$f <- nn_linear(7,1)
self$g <- nn_sigmoid()
},
forward = function(x) {
x %>%
self$f() %>%
self$g()
}
)
f <- logistic()
f(X)
Loss <- function(X, y, Fun){
nn_bce_loss()(Fun(X), y)
}
f <- logistic()
optimizer <- optim_adam(f$parameters, lr = 0.01)
n <- 1000
for (i in 1:n){
loss <- Loss(X, y, f)
optimizer$zero_grad()
loss$backward()
optimizer$step()
if (i %% 100 == 0) {
cat(sprintf('Epoch: %d, Loss: %.6f\n', i, loss$item()))
}
}
predicted_probabilities <- f(X) %>% as_array()
torch_predictions <- ifelse(predicted_probabilities >= 0.5, 1, 0)
torch_overview <- overview(torch_predictions, df$y)
torch_overview
summary_table <- rbind(fullModel_overview, stepwise_overview, lasso_overview,
torch_overview) %>%
mutate(Model = c('Full Model', 'Stepwise Regression', 'Lasso', 'Torch'))
summary_table <- summary_table[,c('Model', 'accuracy', 'error', 'false_positive_rate','false_negative_rate')]
summary_table
sessionInfo()
